{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650366d9",
   "metadata": {},
   "source": [
    "# Creating a model and training it for generating and clustering music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df2bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create models: VAE, WAE, U-NET\n",
    "#Save model weights\n",
    "#Evaluate in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760ad3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmus/.local/lib/python3.10/site-packages/deeptrack/__init__.py:14: UserWarning: TensorFlow is detected in your environment. DeepTrack2 version 2.0++ no longer supports TensorFlow. If you need TensorFlow support, please install the legacy version 1.7 of DeepTrack2:\n",
      "\n",
      "    pip install deeptrack==1.7\n",
      "\n",
      "For more details, refer to the DeepTrack documentation.\n",
      "  warnings.warn(\n",
      "WARNING:pint.util:Redefining '[magnetic_flux]' (<class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'>)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce431f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 24984\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data\n",
    "data_dir = os.path.expanduser(\"./trainImages\")\n",
    "\n",
    "#Load image files using ImageFolder\n",
    "trainFiles = dt.sources.ImageFolder(root=data_dir)\n",
    "\n",
    "print(f\"Number of train images: {len(trainFiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55800ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create image pipeline\n",
    "class CropWidth:\n",
    "    def __init__(self, target_width):\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        # assuming input shape [C, H, W]\n",
    "        return x[..., :self.target_width]\n",
    "    \n",
    "image_pip = (dt.LoadImage(trainFiles.path) >> dt.NormalizeMinMax()\n",
    "             >> dt.MoveAxis(2, 0) >> dt.pytorch.ToTensor(dtype=torch.float) >> CropWidth(1904))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20345905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of each image is: torch.Size([1, 128, 1904])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = image_pip(trainFiles.path[0])\n",
    "print(f\"The size of each image is: {img_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30890ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoEncoder(\n",
      "  (encoder): ConvolutionalEncoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=974848, out_features=20, bias=True)\n",
      "  (fc_var): Linear(in_features=974848, out_features=20, bias=True)\n",
      "  (fc_dec): Linear(in_features=20, out_features=974848, bias=True)\n",
      "  (decoder): ConvolutionalDecoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (layer): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (layer): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (preprocess): Unflatten(dim=1, unflattened_size=(64, 32, 476))\n",
      "  )\n",
      "  (reconstruction_loss): BCELoss()\n",
      "  (train_metrics): MetricCollection,\n",
      "    prefix=train\n",
      "  )\n",
      "  (val_metrics): MetricCollection,\n",
      "    prefix=val\n",
      "  )\n",
      "  (test_metrics): MetricCollection,\n",
      "    prefix=test\n",
      "  )\n",
      "  (optimizer): SGD[SGD](lr=0.01, momentum=0.9)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#VAE\n",
    "from deeplay import SGD\n",
    "import torch\n",
    "#Dont use reduction=\"sum\" gradients becomes giant and memory issues are made worse. //BD\n",
    "vae = dl.VariationalAutoEncoder(input_size=(128,1907),\n",
    "    latent_dim=20, channels=[32, 64],\n",
    "    reconstruction_loss=torch.nn.BCELoss(reduction=\"mean\"), beta=1e-3, optimizer=SGD(lr=0.01, momentum=0.9)\n",
    ").create()\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giovanni used image_pip & image_pip here, do not really know why // L-Thor\n",
    "#I think it is because one is used for the mean and one is used for the variance //BD\n",
    "train_dataset = dt.pytorch.Dataset(image_pip & image_pip, inputs=trainFiles)\n",
    "train_loader = dl.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955df7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmus/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "2025-04-28 21:09:55.573530: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 21:09:55.582125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745867395.590711   14134 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745867395.593594   14134 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745867395.601972   14134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745867395.601981   14134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745867395.601983   14134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745867395.601983   14134 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-28 21:09:55.604789: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  | Name                | Type                   | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | encoder             | ConvolutionalEncoder2d | 55.7 K | train\n",
      "1 | fc_mu               | Linear                 | 19.5 M | train\n",
      "2 | fc_var              | Linear                 | 19.5 M | train\n",
      "3 | fc_dec              | Linear                 | 20.5 M | train\n",
      "4 | decoder             | ConvolutionalDecoder2d | 76.3 K | train\n",
      "5 | reconstruction_loss | BCELoss                | 0      | train\n",
      "6 | train_metrics       | MetricCollection       | 0      | train\n",
      "7 | val_metrics         | MetricCollection       | 0      | train\n",
      "8 | test_metrics        | MetricCollection       | 0      | train\n",
      "9 | optimizer           | SGD                    | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "59.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "59.6 M    Total params\n",
      "238.391   Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/rasmus/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▍         | 36/781 [00:09<03:11,  3.89it/s, v_num=19, train_rec_loss_step=0.673, train_KL_step=0.0173, train_total_loss_step=0.673]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmus/.local/lib/python3.10/site-packages/deeptrack/math.py:156: RuntimeWarning: invalid value encountered in divide\n",
      "  image = image / np.ptp(image) * (max - min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 781/781 [02:26<00:00,  5.32it/s, v_num=19, train_rec_loss_step=0.669, train_KL_step=0.00453, train_total_loss_step=0.669, train_rec_loss_epoch=0.674, train_KL_epoch=0.00727, train_total_loss_epoch=0.674]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: Training works, no errors, but it is VERY slow and my computer semi-freezes when I do it, perhaps something is wrong,\n",
    "#or my computer is trash or maybe we just have to thug it out // L-Thor\n",
    "#Memory issues, it seems to not properly reuse memory after each batch. Larger batches uses less memory which makes no sense to something\n",
    "# s definetly wrong //BD\n",
    "vae_trainer = dl.Trainer(max_epochs=2, accelerator=\"auto\")\n",
    "vae_trainer.fit(vae, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c879635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAE, let us just focus on the VAE for now, later we can use this code\n",
    "wae = dl.WassersteinAutoEncoder(\n",
    "    channels=[32, 64, 128], latent_dim=20,\n",
    "    reconstruction_loss=torch.nn.MSELoss(reduction=\"mean\"),\n",
    ").create()\n",
    "\n",
    "#print(wae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Guessed Channel Layout for Input Stream #0.0 : mono\n",
      "Input #0, wav, from '/home/rasmus/Python/MusicGenerationD3.1/temp_output.wav':\n",
      "  Duration: 00:00:29.95, bitrate: 705 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> mp3 (libmp3lame))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp3, to '/home/rasmus/Python/MusicGenerationD3.1/testing.mp3':\n",
      "  Metadata:\n",
      "    TSSE            : Lavf58.76.100\n",
      "  Stream #0:0: Audio: mp3, 44100 Hz, mono, s16p\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libmp3lame\n",
      "size=     750kB time=00:00:29.93 bitrate= 205.3kbits/s speed= 244x    \n",
      "video:0kB audio:750kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.029552%\n"
     ]
    }
   ],
   "source": [
    "#Example on how to generate music, just random noice now but with proper training and clustering we could try to sample things adjacent to a genre //BD\n",
    "\n",
    "import torch\n",
    "from Image2Sound import Image2Sound, SaveAudio\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "vae.eval()  \n",
    "n_samples = 1 \n",
    "latent_dim = 20  \n",
    "#Had to add a scalar to make the volume higher, think because of the normalization in the pipeline //BD\n",
    "z = 255*torch.randn(n_samples, latent_dim).to(next(vae.parameters()).device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_image = vae.decode(z)\n",
    "    #TODO: Change the Image2Sound so we don't have to define conf again here when generating, should be a simple enough fix. Alternatively just make a file holding the class\n",
    "    #with a proper init //BD\n",
    "    class conf:\n",
    "        sampling_rate = 44100\n",
    "        duration = 30\n",
    "        hop_length = 694\n",
    "        fmin = 20\n",
    "        fmax = sampling_rate // 2\n",
    "        n_mels = 128\n",
    "        n_fft = n_mels * 20\n",
    "        samples = sampling_rate * duration\n",
    "    save_image(generated_image, 'generated_sample.jpg')\n",
    "    audio = Image2Sound('generated_sample.jpg', conf)\n",
    "    SaveAudio(audio,os.getcwd(),\"testing.mp3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
