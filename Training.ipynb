{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650366d9",
   "metadata": {},
   "source": [
    "# Creating a model and training it for generating and clustering music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create models: VAE, WAE, U-NET\n",
    "#Save model weights\n",
    "#Evaluate in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ad3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce431f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data\n",
    "data_dir = os.path.expanduser(\"./trainImages\")\n",
    "\n",
    "#Load image files using ImageFolder\n",
    "trainFiles = dt.sources.ImageFolder(root=data_dir)\n",
    "\n",
    "print(f\"Number of train images: {len(trainFiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55800ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create image pipeline\n",
    "class CropWidth:\n",
    "    def __init__(self, target_width):\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        # assuming input shape [C, H, W]\n",
    "        return x[..., :self.target_width]\n",
    "    \n",
    "image_pip = (dt.LoadImage(trainFiles.path) >> dt.NormalizeMinMax()\n",
    "             >> dt.MoveAxis(2, 0) >> dt.pytorch.ToTensor(dtype=torch.float) >> CropWidth(2560\n",
    "             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20345905",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = image_pip(trainFiles.path[0])\n",
    "print(f\"The size of each image is: {img_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30890ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAE\n",
    "from deeplay import AdamW\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#Dont use reduction=\"sum\" gradients becomes giant and memory issues are made worse. //BD\n",
    "vae = dl.VariationalAutoEncoder(input_size=(256,2560),\n",
    "    latent_dim=10, channels=[64, 128, 256, 512],\n",
    "    reconstruction_loss=torch.nn.BCELoss(reduction=\"mean\"), beta=1, optimizer=AdamW(lr=1e-3)\n",
    ").create()\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giovanni used image_pip & image_pip here, do not really know why // L-Thor\n",
    "#I think it is because one is used for the mean and one is used for the variance //BD\n",
    "import dataset\n",
    "train_dataset = dataset.Dataset(image_pip & image_pip, inputs=trainFiles)\n",
    "print(train_dataset)\n",
    "train_loader = dl.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955df7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Training works, no errors, but it is VERY slow and my computer semi-freezes when I do it, perhaps something is wrong,\n",
    "#or my computer is trash or maybe we just have to thug it out // L-Thor\n",
    "#Memory issues, it seems to not properly reuse memory after each batch. Larger batches uses less memory which makes no sense to something\n",
    "# s definetly wrong //BD\n",
    "vae_trainer = dl.Trainer(max_epochs=10, accelerator=\"auto\")\n",
    "vae_trainer.fit(vae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f31c89",
   "metadata": {},
   "source": [
    "### Plotting the training progress and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7aca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vae_trainer.print()\n",
    "torch.save(vae.state_dict(), \"vae.pth\")  #Saving the model.\n",
    "#vae.load_state_dict(torch.load(\"vae.pth\"))  // Use this to load the model! //BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c879635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAE, let us just focus on the VAE for now, later we can use this code\n",
    "wae = dl.WassersteinAutoEncoder(\n",
    "    channels=[32, 64, 128], latent_dim=20,\n",
    "    reconstruction_loss=torch.nn.MSELoss(reduction=\"mean\"),\n",
    ").create()\n",
    "\n",
    "#print(wae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on how to generate music, just random noice now but with proper training and clustering we could try to sample things adjacent to a genre //BD\n",
    "\n",
    "import torch\n",
    "from Image2Sound import Image2Sound, SaveAudio\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "vae.eval()  \n",
    "n_samples = 1 \n",
    "latent_dim = 20  \n",
    "#Had to add a scalar to make the volume higher, think because of the normalization in the pipeline //BD\n",
    "z = 255*torch.randn(n_samples, latent_dim).to(next(vae.parameters()).device)\n",
    "print(z.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_image = vae.decode(z)\n",
    "    #TODO: Change the Image2Sound so we don't have to define conf again here when generating, should be a simple enough fix. Alternatively just make a file holding the class\n",
    "    #with a proper init //BD\n",
    "    class conf:\n",
    "        sampling_rate = 44100\n",
    "        duration = 30\n",
    "        hop_length = 694\n",
    "        fmin = 20\n",
    "        fmax = sampling_rate // 2\n",
    "        n_mels = 128\n",
    "        n_fft = n_mels * 20\n",
    "        samples = sampling_rate * duration\n",
    "    save_image(generated_image, 'generated_sample.jpg')\n",
    "    audio = Image2Sound('generated_sample.jpg', conf)\n",
    "    SaveAudio(audio,os.getcwd(),\"testing.mp3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
