{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650366d9",
   "metadata": {},
   "source": [
    "# Creating a model and training it for generating and clustering music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df2bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create models: VAE, WAE, U-NET\n",
    "#Save model weights\n",
    "#Evaluate in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760ad3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmus/.local/lib/python3.10/site-packages/deeptrack/__init__.py:14: UserWarning: TensorFlow is detected in your environment. DeepTrack2 version 2.0++ no longer supports TensorFlow. If you need TensorFlow support, please install the legacy version 1.7 of DeepTrack2:\n",
      "\n",
      "    pip install deeptrack==1.7\n",
      "\n",
      "For more details, refer to the DeepTrack documentation.\n",
      "  warnings.warn(\n",
      "WARNING:pint.util:Redefining '[magnetic_flux]' (<class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'>)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce431f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 24981\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data\n",
    "data_dir = os.path.expanduser(\"./trainImages\")\n",
    "\n",
    "#Load image files using ImageFolder\n",
    "trainFiles = dt.sources.ImageFolder(root=data_dir)\n",
    "\n",
    "print(f\"Number of train images: {len(trainFiles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55800ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create image pipeline\n",
    "class CropWidth:\n",
    "    def __init__(self, target_width):\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        # assuming input shape [C, H, W]\n",
    "        return x[..., :self.target_width]\n",
    "    \n",
    "image_pip = (dt.LoadImage(trainFiles.path) >> dt.NormalizeMinMax()\n",
    "             >> dt.MoveAxis(2, 0) >> dt.pytorch.ToTensor(dtype=torch.float) >> CropWidth(644))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20345905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of each image is: torch.Size([1, 256, 644])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = image_pip(trainFiles.path[0])\n",
    "print(f\"The size of each image is: {img_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30890ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoEncoder(\n",
      "  (encoder): ConvolutionalEncoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=659456, out_features=20, bias=True)\n",
      "  (fc_var): Linear(in_features=659456, out_features=20, bias=True)\n",
      "  (fc_dec): Linear(in_features=20, out_features=659456, bias=True)\n",
      "  (decoder): ConvolutionalDecoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (layer): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (layer): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (preprocess): Unflatten(dim=1, unflattened_size=(64, 64, 161))\n",
      "  )\n",
      "  (reconstruction_loss): BCELoss()\n",
      "  (train_metrics): MetricCollection,\n",
      "    prefix=train\n",
      "  )\n",
      "  (val_metrics): MetricCollection,\n",
      "    prefix=val\n",
      "  )\n",
      "  (test_metrics): MetricCollection,\n",
      "    prefix=test\n",
      "  )\n",
      "  (optimizer): AdamW[AdamW](lr=0.001)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#VAE\n",
    "from deeplay import AdamW\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "#Dont use reduction=\"sum\" gradients becomes giant and memory issues are made worse. //BD\n",
    "vae = dl.VariationalAutoEncoder(input_size=(256,647),\n",
    "    latent_dim=20, channels=[32, 64],\n",
    "    reconstruction_loss=torch.nn.BCELoss(reduction=\"mean\"), beta=1, optimizer=AdamW(lr=0.001)\n",
    ").create()\n",
    "\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d04c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giovanni used image_pip & image_pip here, do not really know why // L-Thor\n",
    "#I think it is because one is used for the mean and one is used for the variance //BD\n",
    "train_dataset = dt.pytorch.Dataset(image_pip & image_pip, inputs=trainFiles)\n",
    "train_loader = dl.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955df7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmus/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "2025-04-29 18:12:57.932320: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-29 18:12:57.940625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745943177.951133   23528 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745943177.954263   23528 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745943177.963848   23528 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745943177.963859   23528 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745943177.963860   23528 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745943177.963861   23528 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-29 18:12:57.966826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  | Name                | Type                   | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | encoder             | ConvolutionalEncoder2d | 55.7 K | train\n",
      "1 | fc_mu               | Linear                 | 13.2 M | train\n",
      "2 | fc_var              | Linear                 | 13.2 M | train\n",
      "3 | fc_dec              | Linear                 | 13.8 M | train\n",
      "4 | decoder             | ConvolutionalDecoder2d | 76.3 K | train\n",
      "5 | reconstruction_loss | BCELoss                | 0      | train\n",
      "6 | train_metrics       | MetricCollection       | 0      | train\n",
      "7 | val_metrics         | MetricCollection       | 0      | train\n",
      "8 | test_metrics        | MetricCollection       | 0      | train\n",
      "9 | optimizer           | AdamW                  | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "40.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.4 M    Total params\n",
      "161.435   Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/rasmus/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 781/781 [01:47<00:00,  7.23it/s, v_num=50, train_rec_loss_step=0.618, train_KL_step=44.70, train_total_loss_step=45.30, train_rec_loss_epoch=0.604, train_KL_epoch=23.50, train_total_loss_epoch=24.10]       \n"
     ]
    }
   ],
   "source": [
    "#NOTE: Training works, no errors, but it is VERY slow and my computer semi-freezes when I do it, perhaps something is wrong,\n",
    "#or my computer is trash or maybe we just have to thug it out // L-Thor\n",
    "#Memory issues, it seems to not properly reuse memory after each batch. Larger batches uses less memory which makes no sense to something\n",
    "# s definetly wrong //BD\n",
    "vae_trainer = dl.Trainer(max_epochs=25, accelerator=\"auto\")\n",
    "vae_trainer.fit(vae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f31c89",
   "metadata": {},
   "source": [
    "### Plotting the training progress and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7aca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vae_trainer.print()\n",
    "torch.save(vae.state_dict(), \"vae.pth\")  #Saving the model.\n",
    "#vae.load_state_dict(torch.load(\"vae.pth\"))  // Use this to load the model! //BD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c879635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAE, let us just focus on the VAE for now, later we can use this code\n",
    "wae = dl.WassersteinAutoEncoder(\n",
    "    channels=[32, 64, 128], latent_dim=20,\n",
    "    reconstruction_loss=torch.nn.MSELoss(reduction=\"mean\"),\n",
    ").create()\n",
    "\n",
    "#print(wae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on how to generate music, just random noice now but with proper training and clustering we could try to sample things adjacent to a genre //BD\n",
    "\n",
    "import torch\n",
    "from Image2Sound import Image2Sound, SaveAudio\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "vae.eval()  \n",
    "n_samples = 1 \n",
    "latent_dim = 20  \n",
    "#Had to add a scalar to make the volume higher, think because of the normalization in the pipeline //BD\n",
    "z = 255*torch.randn(n_samples, latent_dim).to(next(vae.parameters()).device)\n",
    "print(z.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_image = vae.decode(z)\n",
    "    #TODO: Change the Image2Sound so we don't have to define conf again here when generating, should be a simple enough fix. Alternatively just make a file holding the class\n",
    "    #with a proper init //BD\n",
    "    class conf:\n",
    "        sampling_rate = 44100\n",
    "        duration = 30\n",
    "        hop_length = 694\n",
    "        fmin = 20\n",
    "        fmax = sampling_rate // 2\n",
    "        n_mels = 128\n",
    "        n_fft = n_mels * 20\n",
    "        samples = sampling_rate * duration\n",
    "    save_image(generated_image, 'generated_sample.jpg')\n",
    "    audio = Image2Sound('generated_sample.jpg', conf)\n",
    "    SaveAudio(audio,os.getcwd(),\"testing.mp3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
