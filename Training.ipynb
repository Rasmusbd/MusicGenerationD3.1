{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650366d9",
   "metadata": {},
   "source": [
    "# Creating a model and training it for generating and clustering music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df2bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create models: VAE, WAE, U-NET\n",
    "#Save model weights\n",
    "#Evaluate in this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760ad3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pint.util:Redefining '[magnetic_flux]' (<class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'>)\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce431f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images: 10000\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data\n",
    "data_dir = os.path.expanduser(\"./trainImagesSmall\")\n",
    "\n",
    "#Load image files using ImageFolder\n",
    "#trainFiles = dt.sources.ImageFolder(root=data_dir)\n",
    "trainFilesSmall = dt.sources.ImageFolder(root=data_dir)\n",
    "\n",
    "print(f\"Number of train images: {len(trainFilesSmall)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55800ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create image pipeline\n",
    "class CropWidth:\n",
    "    def __init__(self, target_width):\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        return x[..., :self.target_width]\n",
    "    \n",
    "image_pip = (dt.LoadImage(trainFilesSmall.path) >> dt.NormalizeMinMax()\n",
    "             >> dt.MoveAxis(2, 0) >> dt.pytorch.ToTensor(dtype=torch.float) >> CropWidth(512\n",
    "             ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20345905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of each image is: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "img_tensor = image_pip(trainFilesSmall.path[0])\n",
    "print(f\"The size of each image is: {img_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30890ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoEncoder(\n",
      "  (encoder): ConvolutionalEncoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (3): Conv2dBlock(\n",
      "        (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (layer): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Identity()\n",
      "      )\n",
      "    )\n",
      "    (postprocess): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=524288, out_features=50, bias=True)\n",
      "  (fc_var): Linear(in_features=524288, out_features=50, bias=True)\n",
      "  (fc_dec): Linear(in_features=50, out_features=524288, bias=True)\n",
      "  (decoder): ConvolutionalDecoder2d(\n",
      "    (blocks): LayerList(\n",
      "      (0): Conv2dBlock(\n",
      "        (layer): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (1): Conv2dBlock(\n",
      "        (layer): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (2): Conv2dBlock(\n",
      "        (layer): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (upsample): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (3): Conv2dBlock(\n",
      "        (layer): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "        (activation): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (preprocess): Unflatten(dim=1, unflattened_size=(128, 64, 64))\n",
      "  )\n",
      "  (reconstruction_loss): MSELoss()\n",
      "  (train_metrics): MetricCollection,\n",
      "    prefix=train\n",
      "  )\n",
      "  (val_metrics): MetricCollection,\n",
      "    prefix=val\n",
      "  )\n",
      "  (test_metrics): MetricCollection,\n",
      "    prefix=test\n",
      "  )\n",
      "  (optimizer): AdamW[AdamW](lr=0.001)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#VAE\n",
    "from deeplay import AdamW #Specifically importing AdamW so we can easily change it to something else like SGDM //BD\n",
    "import torch\n",
    "\n",
    "#Even with very small beta the KL loss seems to decrease while the rec loss stays steady, suggesting the network is not able to learn the latent space //BD\n",
    "\n",
    "from variationalAutoEncoder import VariationalAutoEncoder\n",
    "\n",
    "vae = VariationalAutoEncoder(input_size=(512,512),\n",
    "    latent_dim=50, channels=[32, 64, 128],\n",
    "    reconstruction_loss=torch.nn.MSELoss(), beta=1, optimizer=AdamW(lr=1e-3)\n",
    ").create()\n",
    "\n",
    "\n",
    "vae.encoder.blocks[0].layer.kernel_size = (5,5)\n",
    "vae.encoder.blocks[1].layer.kernel_size = (5,5)\n",
    "vae.encoder.blocks[2].layer.kernel_size = (5,5)\n",
    "vae.encoder.blocks[3].layer.kernel_size = (5,5)\n",
    "vae.decoder.blocks[0].layer.kernel_size = (5,5)\n",
    "vae.decoder.blocks[1].layer.kernel_size = (5,5)\n",
    "vae.decoder.blocks[2].layer.kernel_size = (5,5)\n",
    "vae.decoder.blocks[3].layer.kernel_size = (5,5)\n",
    "\n",
    "\n",
    "print(vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d04c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset #Imported modified dataset code\n",
    "#train_dataset = dataset.Dataset(image_pip & image_pip, inputs=trainFiles)\n",
    "train_dataset = dt.pytorch.Dataset(image_pip & image_pip, inputs = trainFilesSmall)\n",
    "train_loader = dl.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955df7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasmus/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "\n",
      "  | Name                | Type                   | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | encoder             | ConvolutionalEncoder2d | 240 K  | train\n",
      "1 | fc_mu               | Linear                 | 26.2 M | train\n",
      "2 | fc_var              | Linear                 | 26.2 M | train\n",
      "3 | fc_dec              | Linear                 | 26.7 M | train\n",
      "4 | decoder             | ConvolutionalDecoder2d | 326 K  | train\n",
      "5 | reconstruction_loss | MSELoss                | 0      | train\n",
      "6 | train_metrics       | MetricCollection       | 0      | train\n",
      "7 | val_metrics         | MetricCollection       | 0      | train\n",
      "8 | test_metrics        | MetricCollection       | 0      | train\n",
      "9 | optimizer           | AdamW                  | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "79.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "79.7 M    Total params\n",
      "318.937   Total estimated model params size (MB)\n",
      "44        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 314/625 [00:42<00:42,  7.34it/s, v_num=304, train_rec_loss_step=0.917, train_KL_penalty_step=2.25e+4, train_total_loss_step=2.25e+4]"
     ]
    }
   ],
   "source": [
    "vae_trainer = dl.Trainer(max_epochs=20, accelerator=\"auto\", accumulate_grad_batches=8)\n",
    "vae_trainer.fit(vae, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f31c89",
   "metadata": {},
   "source": [
    "### Plotting the training progress and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7aca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vae_trainer.print()\n",
    "torch.save(vae.state_dict(), \"vae.pth\")  #Saving the model.\n",
    "#vae.load_state_dict(torch.load(\"vae.pth\"))  // Use this to load the weights into a new model //BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c879635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAE, let us just focus on the VAE for now, later we can use this code\n",
    "\n",
    "wae = dl.WassersteinAutoEncoder(\n",
    "    channels=[32, 64, 128], latent_dim=20,\n",
    "    reconstruction_loss=torch.nn.MSELoss(reduction=\"mean\"),\n",
    ").create()\n",
    "\n",
    "#print(wae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example on how to generate music, just random noice now but with proper training and clustering we could try to sample things adjacent to a genre //BD\n",
    "\"\"\"\n",
    "--------------------------\n",
    "THIS IS OUTDATED DON'T USE\n",
    "--------------------------\n",
    "\"\"\"\n",
    "import torch\n",
    "from Image2Sound import Image2Sound, SaveAudio\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "vae.eval()  \n",
    "n_samples = 1 \n",
    "latent_dim = 20  \n",
    "#Had to add a scalar to make the volume higher, think because of the normalization in the pipeline //BD\n",
    "z = 255*torch.randn(n_samples, latent_dim).to(next(vae.parameters()).device)\n",
    "print(z.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_image = vae.decode(z)\n",
    "    #TODO: Change the Image2Sound so we don't have to define conf again here when generating, should be a simple enough fix. Alternatively just make a file holding the class\n",
    "    #with a proper init //BD\n",
    "    class conf:\n",
    "        sampling_rate = 44100\n",
    "        duration = 30\n",
    "        hop_length = 694\n",
    "        fmin = 20\n",
    "        fmax = sampling_rate // 2\n",
    "        n_mels = 128\n",
    "        n_fft = n_mels * 20\n",
    "        samples = sampling_rate * duration\n",
    "    save_image(generated_image, 'generated_sample.jpg')\n",
    "    audio = Image2Sound('generated_sample.jpg', conf)\n",
    "    SaveAudio(audio,os.getcwd(),\"testing.mp3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
