{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939652a1",
   "metadata": {},
   "source": [
    "# Using the model to cluster songs according to their genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fc88a",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Guessed Channel Layout for Input Stream #0.0 : mono\n",
      "Input #0, wav, from '/home/rasmus/Python/MusicGenerationD3.1/temp_output.wav':\n",
      "  Duration: 00:00:29.95, bitrate: 705 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> mp3 (libmp3lame))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp3, to '/home/rasmus/Python/MusicGenerationD3.1/test_sample.mp3':\n",
      "  Metadata:\n",
      "    TSSE            : Lavf58.76.100\n",
      "  Stream #0:0: Audio: mp3, 44100 Hz, mono, s16p\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libmp3lame\n",
      "size=     382kB time=00:00:29.93 bitrate= 104.6kbits/s speed= 260x    \n",
      "video:0kB audio:382kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.058011%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "import os\n",
    "\n",
    "vae = dl.VariationalAutoEncoder(input_size=(256,2560),\n",
    "    latent_dim=10, channels=[64, 128, 256, 512],\n",
    "    reconstruction_loss=torch.nn.MSELoss(reduction=\"sum\")\n",
    ").create()\n",
    "vae.load_state_dict(torch.load(\"vae.pth\"))\n",
    "vae.eval()\n",
    "\n",
    "#Preprocess data\n",
    "data_dir = os.path.expanduser(\"./trainImages\")\n",
    "\n",
    "#Load image files using ImageFolder\n",
    "trainFiles = dt.sources.ImageFolder(root=data_dir)\n",
    "\n",
    "class CropWidth:\n",
    "    def __init__(self, target_width):\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __call__(self, x: torch.Tensor):\n",
    "        # assuming input shape [C, H, W]\n",
    "        return x[..., :self.target_width]\n",
    "    \n",
    "image_pip = (dt.LoadImage(trainFiles.path) >> dt.NormalizeMinMax()\n",
    "             >> dt.MoveAxis(2, 0) >> dt.pytorch.ToTensor(dtype=torch.float) >> CropWidth(2560))\n",
    "\n",
    "train_dataset = dt.pytorch.Dataset(image_pip & image_pip, inputs=trainFiles)\n",
    "train_loader = dl.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "from Image2Sound import *\n",
    "from torchvision.utils import save_image\n",
    "i = 0\n",
    "for image, _ in train_loader:\n",
    "    mu, _ = vae.encode(image)\n",
    "    image = vae.decode(mu).clone().detach().squeeze(0)\n",
    "    break\n",
    "\n",
    "\"\"\"\n",
    "latentSpaceImage = vae.encode(img_tensor.unsqueeze(0))\n",
    "print(latentSpaceImage.shape)\n",
    "z = 255*torch.stack(latentSpaceImage)\n",
    "print(z)\n",
    "\"\"\"\n",
    "iamge = image.float()\n",
    "class conf:\n",
    "    sampling_rate = 44100\n",
    "    duration = 30\n",
    "    samples = sampling_rate * duration\n",
    "    n_mels = 256\n",
    "    hop_length = 512\n",
    "    n_fft = 2048 \n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "\n",
    "save_image(image, 'test_sample.jpg')\n",
    "audio = Image2Sound('test_sample.jpg', conf)\n",
    "SaveAudio(audio,os.getcwd(),\"test_sample.mp3\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
